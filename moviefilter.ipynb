{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Understanding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_users = pd.read_csv('C:/Users/dalle/OneDrive/Desktop/Dallen/BYU/Winter 2025/Intex2/Machine Learning/movies_users.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_triple = pd.read_csv('C:/Users/dalle/OneDrive/Desktop/Dallen/BYU/Winter 2025/Intex2/Machine Learning/movies_ratings.csv')\n",
    "df_triple.sort_values(by=['user_id', 'show_id', 'rating'], inplace=True) # This sorting will matter later when we clean the data\n",
    "value_counts = df_triple['show_id'].value_counts()\n",
    "keep_list = value_counts[value_counts >= 1]\n",
    "df_triple_filtered = df_triple.loc[df_triple['show_id'].isin(keep_list.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_movies = pd.read_csv('C:/Users/dalle/OneDrive/Desktop/Dallen/BYU/Winter 2025/Intex2/Machine Learning/movies_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('C:/Users/dalle/OneDrive/Desktop/Dallen/BYU/Winter 2025/Intex2/Machine Learning/movies_titles.csv')\n",
    "print(df.isna().sum(), '\\n')\n",
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Cleaning/Automation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##translate to english here\n",
    "\n",
    "df.director.fillna('placeholder123', inplace=True)\n",
    "df.cast.fillna('placeholder123', inplace=True)\n",
    "df.country.fillna('placeholder123', inplace=True)\n",
    "df.rating.fillna('placeholder123', inplace=True)\n",
    "df.duration.fillna('placeholder123', inplace=True)\n",
    "\n",
    "df.drop_duplicates(inplace=True)\n",
    "\n",
    "# Very important step\n",
    "df.reset_index(inplace=True)\n",
    "\n",
    "print(df.isna().sum(), '\\n')\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content Filtering Algorithm**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer, ENGLISH_STOP_WORDS\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Create a TF-IDF vectorizer and compute the TF-IDF matrix\n",
    "custom_stop_words = ENGLISH_STOP_WORDS.union({'placeholder123', 'season', 'episode', 'series', 'film', 'movie', 'based'})\n",
    "tfidf = TfidfVectorizer(stop_words=list(custom_stop_words))\n",
    "df['combined'] = df['description'].fillna('') + ' ' + df['director'].fillna('') + ' ' + df['type'].fillna('') + ' ' + df['rating'].fillna('')\n",
    "tfidf_matrix = tfidf.fit_transform(df['combined'])\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
    "\n",
    "# Define the RecommendationSystem class (if not already defined)\n",
    "class RecommendationSystem:\n",
    "    def __init__(self, sim_matrix, df):\n",
    "        self.sim_matrix = sim_matrix\n",
    "        self.df = df\n",
    "\n",
    "# Initialize the recommendation system\n",
    "rec_system = RecommendationSystem(cosine_sim, df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content Filtering (movies like this)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "# Create a list to store the recommendations\n",
    "recommendations_list = []\n",
    "\n",
    "# Create a mapping between content IDs and their positions in the similarity matrix\n",
    "unique_show_ids = rec_system.df['show_id'].unique()\n",
    "show_id_to_index = {id: idx for idx, id in enumerate(unique_show_ids)}\n",
    "index_to_show_id = {idx: id for idx, id in enumerate(unique_show_ids)}\n",
    "\n",
    "# Modify your get_recommendations function to use the mapping \n",
    "def get_mapped_recommendations(show_id, n=10, content_type=None):\n",
    "    try:\n",
    "        # Convert content ID to matrix index\n",
    "        if show_id not in show_id_to_index:\n",
    "            print(f\"Item {show_id} is not in the similarity matrix you provided\")\n",
    "            return None\n",
    "            \n",
    "        matrix_idx = show_id_to_index[show_id]\n",
    "        \n",
    "        # Get similarity scores\n",
    "        sim_scores = list(enumerate(rec_system.sim_matrix[matrix_idx]))\n",
    "        \n",
    "        # Sort the items based on similarity scores\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if content_type:\n",
    "        # Skip the first one (itself) and collect items matching the requested type\n",
    "            filtered_scores = []\n",
    "            for idx, score in sim_scores[1:]:\n",
    "                if df.loc[idx, 'type'] == content_type:\n",
    "                    filtered_scores.append((idx, score))\n",
    "            \n",
    "            # Break once we have enough recommendations\n",
    "                if len(filtered_scores) >= n:\n",
    "                    break\n",
    "        \n",
    "            sim_scores = filtered_scores[:n]  # Take at most n items\n",
    "        else:\n",
    "            # Get the scores of the n most similar items; start at 1 so that it skips itself\n",
    "            sim_scores = sim_scores[1:n+1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Get the item indices\n",
    "        item_indices = [i[0] for i in sim_scores]\n",
    "        \n",
    "        # Map indices back to content IDs\n",
    "        recommended_ids = [index_to_show_id[idx] for idx in item_indices]\n",
    "        \n",
    "        return recommended_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content ID {show_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through all content IDs in the dataset\n",
    "for show_id in unique_show_ids:\n",
    "    # Get recommendations for the current content ID\n",
    "    recommended_ids = get_mapped_recommendations(show_id, n=10, content_type='Movie')\n",
    "    \n",
    "    if recommended_ids is not None:\n",
    "        # Ensure there are exactly 5 recommendations (fill with empty strings if fewer)\n",
    "        while len(recommended_ids) < 5:\n",
    "            recommended_ids.append(\"\")\n",
    "        \n",
    "        # Add the content ID and its recommendations to the list\n",
    "        recommendations_list.append({\n",
    "            'showId': show_id,\n",
    "            'recommendation_1': recommended_ids[0],\n",
    "            'recommendation_2': recommended_ids[1],\n",
    "            'recommendation_3': recommended_ids[2],\n",
    "            'recommendation_4': recommended_ids[3],\n",
    "            'recommendation_5': recommended_ids[4],\n",
    "            'recommendation_6': recommended_ids[5], \n",
    "            'recommendation_7': recommended_ids[6],\n",
    "            'recommendation_8': recommended_ids[7],\n",
    "            'recommendation_9': recommended_ids[8],\n",
    "            'recommendation_10': recommended_ids[9]\n",
    "        })\n",
    "\n",
    "# Convert the recommendations list to a DataFrame\n",
    "recommendations_df = pd.DataFrame(recommendations_list)\n",
    "\n",
    "# Save the recommendations DataFrame to a CSV file\n",
    "conn = sqlite3.connect('MovieRecommendByShowID.sqlite')\n",
    "recommendations_df.to_sql('movieRecommendations', conn, index=False, if_exists='replace')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Content Filtering (tv shows like this)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "# Create a list to store the recommendations\n",
    "recommendations_list = []\n",
    "\n",
    "# Create a mapping between content IDs and their positions in the similarity matrix\n",
    "unique_show_ids = rec_system.df['show_id'].unique()\n",
    "show_id_to_index = {id: idx for idx, id in enumerate(unique_show_ids)}\n",
    "index_to_show_id = {idx: id for idx, id in enumerate(unique_show_ids)}\n",
    "\n",
    "# Modify your get_recommendations function to use the mapping \n",
    "def get_mapped_recommendations(show_id, n=10, content_type=None):\n",
    "    try:\n",
    "        # Convert content ID to matrix index\n",
    "        if show_id not in show_id_to_index:\n",
    "            print(f\"Item {show_id} is not in the similarity matrix you provided\")\n",
    "            return None\n",
    "            \n",
    "        matrix_idx = show_id_to_index[show_id]\n",
    "        \n",
    "        # Get similarity scores\n",
    "        sim_scores = list(enumerate(rec_system.sim_matrix[matrix_idx]))\n",
    "        \n",
    "        # Sort the items based on similarity scores\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if content_type:\n",
    "        # Skip the first one (itself) and collect items matching the requested type\n",
    "            filtered_scores = []\n",
    "            for idx, score in sim_scores[1:]:\n",
    "                if df.loc[idx, 'type'] == content_type:\n",
    "                    filtered_scores.append((idx, score))\n",
    "            \n",
    "            # Break once we have enough recommendations\n",
    "                if len(filtered_scores) >= n:\n",
    "                    break\n",
    "        \n",
    "            sim_scores = filtered_scores[:n]  # Take at most n items\n",
    "        else:\n",
    "            # Get the scores of the n most similar items; start at 1 so that it skips itself\n",
    "            sim_scores = sim_scores[1:n+1]\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Get the item indices\n",
    "        item_indices = [i[0] for i in sim_scores]\n",
    "        \n",
    "        # Map indices back to content IDs\n",
    "        recommended_ids = [index_to_show_id[idx] for idx in item_indices]\n",
    "        \n",
    "        return recommended_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content ID {show_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through all content IDs in the dataset\n",
    "for show_id in unique_show_ids:\n",
    "    # Get recommendations for the current content ID\n",
    "    recommended_ids = get_mapped_recommendations(show_id, n=10, content_type='TV Show')\n",
    "    \n",
    "    if recommended_ids is not None:\n",
    "        # Ensure there are exactly 5 recommendations (fill with empty strings if fewer)\n",
    "        while len(recommended_ids) < 5:\n",
    "            recommended_ids.append(\"\")\n",
    "        \n",
    "        # Add the content ID and its recommendations to the list\n",
    "        recommendations_list.append({\n",
    "            'showId': show_id,\n",
    "            'recommendation_1': recommended_ids[0],\n",
    "            'recommendation_2': recommended_ids[1],\n",
    "            'recommendation_3': recommended_ids[2],\n",
    "            'recommendation_4': recommended_ids[3],\n",
    "            'recommendation_5': recommended_ids[4],\n",
    "            'recommendation_6': recommended_ids[5], \n",
    "            'recommendation_7': recommended_ids[6],\n",
    "            'recommendation_8': recommended_ids[7],\n",
    "            'recommendation_9': recommended_ids[8],\n",
    "            'recommendation_10': recommended_ids[9]\n",
    "        })\n",
    "\n",
    "# Convert the recommendations list to a DataFrame\n",
    "recommendations_df = pd.DataFrame(recommendations_list)\n",
    "\n",
    "# Save the recommendations DataFrame to a CSV file\n",
    "conn = sqlite3.connect('TVRecommendByShowID.sqlite')\n",
    "recommendations_df.to_sql('tvRecommendations', conn, index=False, if_exists='replace')\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Filter by User ID**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# Create a list to store the recommendations\n",
    "recommendations_list = []\n",
    "\n",
    "# Create a mapping between content IDs and their positions in the similarity matrix\n",
    "unique_show_ids = rec_system.df['show_id'].unique()\n",
    "show_id_to_index = {id: idx for idx, id in enumerate(unique_show_ids)}\n",
    "index_to_show_id = {idx: id for idx, id in enumerate(unique_show_ids)}\n",
    "\n",
    "# Modify your get_recommendations function to use the mapping \n",
    "def get_mapped_recommendations(show_id, n=10, content_type=None):\n",
    "    try:\n",
    "        # Convert content ID to matrix index\n",
    "        if show_id not in show_id_to_index:\n",
    "            print(f\"Item {show_id} is not in the similarity matrix you provided\")\n",
    "            return None\n",
    "            \n",
    "        matrix_idx = show_id_to_index[show_id]\n",
    "        \n",
    "        # Get similarity scores\n",
    "        sim_scores = list(enumerate(rec_system.sim_matrix[matrix_idx]))\n",
    "        \n",
    "        # Sort the items based on similarity scores\n",
    "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        if content_type:\n",
    "            # Skip the first one (itself) and collect items matching the requested type\n",
    "            filtered_scores = []\n",
    "            for idx, score in sim_scores[1:]:\n",
    "                if df.loc[idx, 'type'] == content_type:\n",
    "                    filtered_scores.append((idx, score))\n",
    "                \n",
    "                # Break once we have enough recommendations\n",
    "                if len(filtered_scores) >= n:\n",
    "                    break\n",
    "        \n",
    "            sim_scores = filtered_scores[:n]  # Take at most n items\n",
    "        else:\n",
    "            # Get the scores of the n most similar items; start at 1 so that it skips itself\n",
    "            sim_scores = sim_scores[1:n+1]\n",
    "        \n",
    "        # Get the item indices\n",
    "        item_indices = [i[0] for i in sim_scores]\n",
    "        \n",
    "        # Map indices back to content IDs\n",
    "        recommended_ids = [index_to_show_id[idx] for idx in item_indices]\n",
    "        \n",
    "        return recommended_ids\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing content ID {show_id}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Iterate through each user to get recommendations based on their top-rated movies\n",
    "for user_id in df_triple_filtered['user_id'].unique():\n",
    "    # Filter the data by only those movies rated by this user\n",
    "    df_user_ratings = df_triple_filtered[df_triple_filtered['user_id'] == user_id]\n",
    "\n",
    "    # Find the maximum rating for this user\n",
    "    max_rating = df_user_ratings['rating'].max()\n",
    "\n",
    "    # Get all movies with the max rating for the user (some users may have multiple)\n",
    "    df_favorites = df_user_ratings[df_user_ratings['rating'] == max_rating]['show_id']\n",
    "\n",
    "    # For each top-rated movie, get recommendations\n",
    "    for movie_id in df_favorites:\n",
    "        # Get recommendations based on the top-rated movie\n",
    "        recommended_ids = get_mapped_recommendations(movie_id, n=10)\n",
    "\n",
    "        if recommended_ids is not None:\n",
    "            # Ensure there are exactly 10 recommendations (fill with empty strings if fewer)\n",
    "            while len(recommended_ids) < 10:\n",
    "                recommended_ids.append(\"\")\n",
    "\n",
    "            # Add the user's recommendations to the list\n",
    "            recommendations_list.append({\n",
    "                'user_id': user_id,\n",
    "                'show_id': movie_id,\n",
    "                'recommendation_1': recommended_ids[0],\n",
    "                'recommendation_2': recommended_ids[1],\n",
    "                'recommendation_3': recommended_ids[2],\n",
    "                'recommendation_4': recommended_ids[3],\n",
    "                'recommendation_5': recommended_ids[4],\n",
    "                'recommendation_6': recommended_ids[5],\n",
    "                'recommendation_7': recommended_ids[6],\n",
    "                'recommendation_8': recommended_ids[7],\n",
    "                'recommendation_9': recommended_ids[8],\n",
    "                'recommendation_10': recommended_ids[9]\n",
    "            })\n",
    "\n",
    "# Convert the recommendations list to a DataFrame\n",
    "recommendations_df = pd.DataFrame(recommendations_list)\n",
    "\n",
    "# Save the recommendations DataFrame to a SQLite database\n",
    "conn = sqlite3.connect('ShowRecommendByUserID.sqlite')\n",
    "recommendations_df.to_sql('userRecommendations', conn, index=False, if_exists='replace')\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "numpy2env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
